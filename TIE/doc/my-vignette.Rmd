---
title: "Include all my work"
author: "18070"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = TRUE
)
set.seed(1)
```


##This is the work in 2018.9.14

###Assignment description

Write a .Rmd file to implement at least three examples of different types in the above books (texts, numerical, tables, and figures)

*example 1*

this example is from page 5 from book "R for beginners"

```{r example1}
n = 3+ rnorm(1)
n
```
rnorm(1) generates a random number which is a Gaussian random number, so the result depends on the computer.

*example 2*

this example is from page 10 from the book

```{r example2}
x = "Double quotes\" delimitate R's strings"
x
cat(x)
```

this example reminds me of one bug I met when I read data from disk.
I write something like the things below.

```{r myexample, eval = FALSE}
data = read.table("C:\Users\lenovo\Desktop\...")
```
But it cannot be compiled. I googled for solution. It turns out that I should add \\ in every \\, in this way, the R will take \\ as \\ but not something else.

*example 3*

I am quite confused with the word tables. In this text, I will think it as data frame.

```{r example3}
a = expand.grid(h = c(60, 80), w = c(100, 300), sex = c("Male", "Female"))
```

This funcionn will generate a dataframe, and the results will conclude every match of every attribute.
And if you are worried that whether it is a dataframe you can check that like this:

```{r example continue}
class(a)
```

And last I will plot a figure.

*example 4*

```{r example4}
x = rnorm(10)
y = rnorm(10)
plot(x,y)
```

Autually there are some details in the plot() function. But I think these can be learned when used.


##This is the work in 2018.9.21

### assigment 3.5
*Problem Discription*
A discrete random variable X has probability mass function

|x|0|1|2|3|4|
|-|:-:|:-:|:-:|:-:|:-:|
|p(x)|0.1|0.2|0.2|0.2|0.3|
Use the inverse transform method to geneate a random sample of size 1000 from the distribution of X. Construct a relateive frequency table and compare the empirical with the theoretical prbabilities. Repeat using the R sample funtion.

*Answer*

*1.Using inverse transform method *
```{r assignment 3.5}
x = 0:4
p = c(1,2,2,2,3)/10
cp = cumsum(p)
m = 1e3
r = numeric(m)
r = x[findInterval(runif(m), cp)+1]
## construct a relative frequency table
table(r)/m
a = as.vector(table(r))
## compare the empirical with the theretical probabilities
a/sum(a)/p
```

*2.repeating using the R sample function*
```{r assignment3.5 continue}
## repeat using the R sample function
b = sample(0:4, m, prob = p, replace = TRUE)
table2 = table(b)
#relative frequency table
table2
c = as.vector(table2)
# compare
c/sum(c)/p
```

*3. analyze*

First, I want to make a comment about the function findInterval()

```{r findInterval()}
a = c(-1,1,3)
m = c(-2,-1, 4)
findInterval(m,a)
```

Using the above code , we can conclude how the findInterval(par1, par2) works. First, it divides the the real line by the number in par2, and denote the first interval as 0, second interval as 1 and so on. Then compare the elements in par1 to to the element in par2, and allocate an interval to each element in par1. And since it denotes the first interval as 0, so when we implement the inverse transform method, we must add 1 to the result returned by the findInterval function. 
There is one more detail, the interval findInterval() gives is a left close,right open interval.

And the compare shows that our inverse transformation does generate the discrete random variable the problem gives, and it shows comparative results to the sample() function.

###Assignment 3.7
*Problem Discription*

Write a function to generate a random sample of size n from the Beta(a,b) distribution by the acceptance-rejection method. Generate a random sample of size 1000 from the Beta(3,2) distribution. Graph the histogrm of the sample with the theoretical Beta(3,2) density superimposed.

*Answer*

*1. random generator*
```{r random_generator}
n = 1000
k = 0
j = 0
y = numeric(n)
while(k<n){
  j = j+1
  u = runif(1)
  x = runif(1)
  if(x^2*(1-x)>u){
    k = k+1
    y[k] = x
  }
}
j
```

*2. Graph*
```{r plot}
plot1 = hist(y, plot = F)
z = rbeta(1000, 3, 2)
plot2 = hist(z, plot = F)
plot(plot1)
plot(plot2,col = rgb(1,0,1,1/4),add = T)
```

*3. Analyze*

First about this code, I think the most tricky part is the plot part. I googled for solution about how to plot two histogram in one part. And the code is the solution I find. I only need to change the color and add the "add = T", so I can plot two histogram in one figure.

Next about the algorithm.
We know that the density function of Beta(a,b) distribution is $\frac{\Gamma(a+b)}{\Gamma(a)*\Gamma(b)}*x^{a-1}*(1-x)^{b-1}$
And so the the coefficient in Beta(3,2) is: $\frac{\Gamma(5)}{\Gamma(3)*\Gamma*(2)}$. And $\frac{\Gamma(5)}{\Gamma(3)*\Gamma*(2)} = 12$, so let $f(x)$ be the density of Beta(3,2) and $g(x)$ be the density of our proposl function(in our case $g(x) = 1$), we have that  $\frac{f(x)}{g(x)}\leq12$. If we let c = 12, we will conjecture that in order to get 1000 random numbers follows Beta(3,2), we will have to generate 12000 random numbers, and the result in part1 verifies my assumption.

In order to get more efficient samplint, we can refine the c, i.e we find the true superium of the Beta(3,2) density function.
After easy calculation, we find that $x^{2}*(1-x)\leq \frac{4}{9}$ So $c = 12*\frac{4}{9} = \frac{16}{3}$, wo in this case, in order to generate 1000 Beta(3,2) random numbers, we will need approximately 5300 random numbers.
Now we use one simulation to verify it.

```{r more efficient generator}
n = 1000
k = 0
j = 0
y = numeric(n)
while(k<n){
  j = j+1
  u = runif(1)
  x = runif(1)
  if(x^2*(1-x)>4/9*u){
    k = k+1
    y[k] = x
  }
}
j
```

Now we verify we do generate Beta(3,2) random numbers.

```{r histogram}
plot1 = hist(y, plot = F)
z = rbeta(1000, 3, 2)
plot2 = hist(z, plot = F)
plot(plot1)
plot(plot2,col = rgb(1,0,1,1/4),add = T)
```

###assignment 3.12
*Problem Discription*

Simulate a continous Exponential-Gamma mixture. Suppose that the rate parameter $\Lambda$ has Gamma(r, $\beta$) distribution and Y has Exp($\Lambda$) distribution. That is, (Y|$\Lambda = \lambda$)~$f_Y(y|\lambda)$ = $\lambda*e^{-\lambda*y}$. Generate 1000 random observations from this mixture with r = 4, and $\beta = 2$

*1.generate*
```{r generat}
n = 1000
r=  4
beta = 2
lambda = rgamma(n, r, beta)
x = rexp(n, lambda)
```

*2.verify*
Use exercize3.13, we can verify our simulation through comparison with the Pareto distribution. And we use inverse transform method to generate Pareto distribution

```{r Pareto distribution}
f = function(x){
  2*(1/(1-x)^0.25-1)
}
u = runif(1000)
y = f(u)
figure1 = hist(x, plot = F, breaks = 40)
figure2 = hist(y, plot = F, breaks = 20)
plot(figure1)
plot(figure2, col = 'red', add =T)
```

Through this figure we can conclude that we solve the problem.


#THIS IS THE WORK IN 2018-9-30

I make it very big here, since it's the only assigment I forgot to submit, so I use a bit front here.

###Problem 5.4
*problem discription*

Write a function to compute a Monte Carlo estimate of the Beta(3,2) cdf. and use the function to estimate F(x) for $0.1,0.2,\dots,0.9$. Compare the estimates with the vaues returned by the pbeta function in R.

*problem solving*
```{r 5.4}
f = function(x,m){
  u = runif(m)
  30*mean(x^3*u^2*(1-x*u)^2)
}
x = (1:9)/10
m = 10000
result = c(NULL)
for(i in 1:9){
  result = c(result, f(x[i], m))
}
result
```
compare to the pbeta
```{r }
another = pbeta(x, 3, 3)
result/another
```
*analysis*

Through last result, we can conclude that our monte carlo integration do make a good work.
First I will derive the formula in my code.
denode $y = \frac{t}{x}$
$\int_0^xt^{\alpha-1}*(1-t)^{\beta-1}dt = \int_0^1 x^\alpha*y^{\alpha-1}*(1-xy)^{\beta-1}dy$
And another way to estimate F(x) is through the following code. Assume that we can generate random number which follows beta distribution.
```{r}
z = rbeta(m, 3,3)
dim(x) = length(x)
p = apply(x, MARGIN = 1, FUN = function(x,z){mean(z<x)}, z = z)
result/p
```

###problem 5.9

*problem discription*

The Rayleigh density is $f(x) = \frac{x}{\sigma^2}*e^{-\frac{x^2}{2\sigma^2}}, x\geq0, \sigma>0$.
Implement a function to generate samples from $Rayleigh(\sigma)$ distribution, using antithetic variables. What is  the percent reduction in variance of $\frac{X+X'}{2}$ compared with $\frac{X_1+X_2}{2}$ for independent $X_1, X_2$ ?
*problem solving*

```{r}
f = function(m, sigma){
  u = runif(m/2)
  x = sqrt(-2*sigma^2*log(1-u))
  x_ = sqrt(-2*sigma^2*log(u))
  c(x, x_)
}
m = 10000
sigma = 2
result = f(m, sigma);
X = result[1:(m/2)]
X_ = result[(m/2+1):m]
var1 = sd((X+X_)/2)
X1 = sqrt(-2*sigma^2*log(1-runif(m/2)))
X2 = sqrt(-2*sigma^2*log(1-runif(m/2)))
var2 = sd((X1+X2)/2)
var1/var2
```
*analysis*

Our first variance is approximately 1/5 of the second variance, which shows the power of the antithetic variables. And the following we will show that why this may happen.
$cov(X, X') = E(XX')-E(X)E(X') = 2*\sigma^2E(ln(1-u)*ln(u))-2*\sigma^2*E(lnu)^2, u \sim U(0,1)$
And bellow I will compute the two expectation in the formula.
```{r}
m = 10000
u = runif(m)
cov = 2*((mean(sqrt(log(1-u)*log(u)))-mean(sqrt(-log(u)))^2))
sqrt((4-pi+2*cov)/((4-pi)*2))
```

this is the theoretical result , compare to our empirical result, they are very close. So we can conclude our algorithm is right.

##problem 5.13

*problem discription*

Find two importance functions $f_1 and f_2$ that are supported on $(1, \infty)$ and are 'close' to 
$g(x) = \frac{x^2}{\sqrt{2\pi}}*e^{\frac{-x^2}{2}}, x>1$. Which of your two importance functions produce the smaller variance in estimating $\int_1^\infty \frac{x^2}{\sqrt{2\pi}}*e^{\frac{-x^2}{2}}dx$ by importance sampling.

*problem solving*
```{r}
m = 100000
g = function(x){
  x^2/sqrt(2*pi)*exp(-x^2/2)*(x>1)
}
u = rnorm(m)
simu1 = g(u)/dnorm(u)
u = sqrt(-2*log(runif(m)))
simu2 = g(u)/(u*exp(-u^2/2))
plot(g, xlim = c(1,10))
plot(function(x){1/sqrt(2*pi)*exp(-x^2/2)}, xlim = c(1,10),add = TRUE, col = 'red')
plot(function(x){x*exp(-x^2/2)}, xlim = c(1,10),add = TRUE, col = 'yellow')
sd(simu1)
sd(simu2)
```
*analysis*

I use two f.The first is $f(x) = \frac{1}{\sqrt{2\pi}}*e^{-\frac{x^2}{2}}$ and the second is $f(x) = x*exp(-\frac{x^2}{2})$.
But just use the figure, I cannot find which one is better.Luckily, through the  analysis of variance, we may choose the second one. And by intuition, this is also true since x is more close to constant compare to $x^2$.

###problem 5.14

*problem discription*

Obtain a Monte Carlo extimate of $\int_1^\infty\frac{x^2}{\sqrt{2\pi}}*e^{\frac{-x^2}{2}}dx$ by importance sampling.

*problem solving*
```{r}
m = 10000
g = function(x){
  x^2/sqrt(2*pi)*exp(-x^2/2)*(x>1)
}
u = rnorm(m)
simu1 = g(u)/dnorm(u)
u = sqrt(-2*log(runif(m)))
simu2 = g(u)/(u*exp(-u^2/2))
mean(simu1)
mean(simu2)
```
*analysis*

we can see that the two functions give similar result, but there is still some difference. We will demonstrate the difference in the following.
```{r}
m = 10000
g = function(x){
  x^2/sqrt(2*pi)*exp(-x^2/2)*(x>1)
}
n = 20
theta = matrix(1:(2*n), nrow = 2)
for (i in 1:n){
  u = rnorm(m)
  simu1 = g(u)/dnorm(u)
  theta[1,i] = mean(simu1)
  u = sqrt(-2*log(runif(m)))
  simu2 = g(u)/(u*exp(-u^2/2))
  theta[2,i] = mean(simu2)
}
sd(theta[1,])
sd(theta[2,])
```
through print out the true value, we can also see that the first importance function suffer from more fluctuation.
```{r}
theta[1,]
theta[2,]
```




##This is the work in 2018-10-12

###6.9
*problem discription*

Let X be a non-negative random variable with ?? = E[X] < ??. For a random sample $x_1,\ldots,x_n$ from the distribution of X, the Gini ratio is de???ned by $G = \frac{1}{2n^2\mu}\sum_{j=1}^n\sum_{i=1}^{n}|x_i-x_j$
The Gini ratio is applied in economics to measure inequality in income distribution. Note that G can be written in terms of the order statistics $x_{(i)}$ as $G=\frac{1}{n^2\mu}\sum_{i=1}^n(2i-n-1)x_{x(i)}$
If the mean is unknown,let $\hat G$be the statistic G with $\mu$ replaced by $\bar x$. Estimate by simulation the mean, median and deciles of $\hat G$ if X is standard lognormal. Repeat the procedure for the uniform distribution and Bernoulli(0.1). Also construct density histograms of the replicates in each case.

*prolem solving*
```{r 6.9}
Gini = function(x){
  x = sort(x)
  x.bar = mean(x)
  n = length(x)
  a = 1:n
  a = 2*a-(n+1)
  G = 1/(n^2*x.bar)*sum(a*x)
  G
}
m = 1e3
n = 1e2
lognormal.simu = uniform.simu = bernoulli.simu = numeric(m)
for(i in 1:m){
  x = rlnorm(n)
  y = runif(n)
  z = sample(0:1, n, replace = TRUE)
  lognormal.simu[i] = Gini(x)
  uniform.simu[i] = Gini(y)
  bernoulli.simu[i] = Gini(z)
}
summary(lognormal.simu)
lognormal.simu = sort(lognormal.simu)
lognormal.simu[((1:9)/10)*m]
plot(density(lognormal.simu))
plot(density(uniform.simu))
plot(density(bernoulli.simu))
```

*analysis*

It's easy to use summary function to give sample mean and some quantile statistics for samples.
And use a function to generate Gini ratio will save our time to compute the Gini ratio.
And to verify our computation. We can easily derive that if $X ~ uniform(0,1)$ then, the Gini ratio is $\frac{1}{3}$.
And through the figure and the summary given below , we can see the density are concentrated on $\frac{1}{3}$
```{r}
summary(uniform.simu)
plot(density(uniform.simu))
```


###6.10
*problem discription*

Construct an approximate 95% confidence interval for the Gini ratio $\gamma = E[G]$.If X is lognormal with unknown parameters. Assess the coverage rate of the estimation procedure with a Monte Carlo experiment.

```{r}
library(boot)
sigma = 1
m = 100
n = 100
boot.Gini = function(x,i) Gini(x[i])
ci.norm<-ci.basic<-ci.perc<-ci.bca<-matrix(NA,m,2) 
for(i in 1:m){
  x = rlnorm(n,0,sigma)
  de = boot(data = x, statistic = boot.Gini, R = 100)
  ci = boot.ci(de,type=c("norm","basic","perc"))
  ci.norm[i,]<-ci$norm[2:3]
  ci.basic[i,]<-ci$basic[4:5]            
  ci.perc[i,]<-ci$percent[4:5]
}
G = (pnorm(0.5, 0, sqrt(0.5))-0.5)*2
cat('norm =',mean(ci.norm[,1]<=G & ci.norm[,2]>=G), 
    'basic =',mean(ci.basic[,1]<=G & ci.basic[,2]>=G), 
    'perc =',mean(ci.perc[,1]<=G & ci.perc[,2]>=G))

```

*analysis*

Since we do not know the distribution of Gini ratio, so we cannot give a parametric confidence interval for Gini ratio. So what I do here, is using the boot function to use bootstrap method to give confidence intervals and the above shows that the confidence interval is too liberal.But if we enlarge n and R, the result will be better, I will illustrate in the following.

```{r}
sigma = 1
m = 100
n = 500
boot.Gini = function(x,i) Gini(x[i])
ci.norm<-ci.basic<-ci.perc<-ci.bca<-matrix(NA,m,2) 
for(i in 1:m){
  x = rlnorm(n,0,sigma)
  de = boot(data = x, statistic = boot.Gini, R = 200)
  ci = boot.ci(de,type=c("norm","basic","perc"))
  ci.norm[i,]<-ci$norm[2:3]
  ci.basic[i,]<-ci$basic[4:5]            
  ci.perc[i,]<-ci$percent[4:5]
}
G = (pnorm(0.5, 0, sqrt(0.5))-0.5)*2
cat('norm =',mean(ci.norm[,1]<=G & ci.norm[,2]>=G), 
    'basic =',mean(ci.basic[,1]<=G & ci.basic[,2]>=G), 
    'perc =',mean(ci.perc[,1]<=G & ci.perc[,2]>=G))
```

Following I will use another way to build confidence interval.


```{r}
mu = 0
sigma = 1
M = 1000
n = 50
Sig1 = Sig2 = Mu_hat = Sigma_hat = result = G1 = G2 = numeric(M)
for (i in 1:M) {
  x = rlnorm(n, mu, sigma)
  y = log(x)
  S = sd(y)
  Sig1[i] = sqrt((n - 1) * S^2 / qchisq(0.025, n - 1, lower.tail = FALSE))
  Sig2[i] = sqrt((n - 1) * S^2 / qchisq(1 - 0.025, n - 1, lower.tail = FALSE))
}

for(i in 1:M){
  G1[i] = (pnorm(Sig1[i]/2, 0, sqrt(0.5))-0.5)*2
  G2[i] = (pnorm(Sig2[i]/2, 0, sqrt(0.5))-0.5)*2
}
G = (pnorm(0.5, 0, sqrt(0.5))-0.5)*2
mean(G1<=G & G2>=G)
```

###6.B

*problem discription*

Tests for association based on Pearson product moment correlation ??, Spearman??s rank correlation coe???cient $\rho_s$, or Kendall??s coe???cient $\tau$, are implemented in cor.test. Show (empirically) that the nonparametric tests based on $\rho_s$ or $\tau$ are less powerful than the correlation test when the sampled distribution is bivariate normal. Find an example of an alternative (a bivariate distribution (X,Y) such that X and Y are dependent) such that at least one of the nonparametric tests have better empirical power than the correlation test against this alternative.

*problem solving*

```{r}
library(MASS)
m = 1e3
Sigma = matrix(c(2,0.2,0.2,1),2,2)
result = matrix(1:(3*m), nrow = 3)
for(i in 1:m){
  mnorm = mvrnorm(n = 100, rep(0,2), Sigma)
  x = mnorm[,1]
  y = mnorm[,2]
  result[1,i] = cor.test(x,y, alternative = "two.sided", method = 'pearson')$p.value
  result[2,i] = cor.test(x, y, alternative = "two.sided", method = 'kendal')$p.value
  result[3,i] = cor.test(x, y, alternative = "two.sided", method = 'spearman')$p.value
}
for(i in 1:3){
  print(mean(result[i,]<=0.05))
}
```

```{r}
for(i in 1:m){
  x = rchisq(100, df = 3)
  y = 1/10*x +9/10*rchisq(100, df = 3)
  result[1,i] = cor.test(x,y, alternative = "two.sided", method = 'pearson')$p.value
  result[2,i] = cor.test(x, y, alternative = "two.sided", method = 'kendal')$p.value
  result[3,i] = cor.test(x, y, alternative = "two.sided", method = 'spearman')$p.value
}
for(i in 1:3){
  print(mean(result[i,]<=0.05))
}
```


*analysis*
It is our common sense that if the samples do follow some parametric distribution, the related test wiill give best test. So if (X,Y) come from two dimensional normal distribution, the pearson test will give the best power. However, when the distribution are far away from normal distribution, the nonparametric method will come into stage. Since the advantages of the nonparametric method is its universality, it does not have any assumption about how the samples are distributed. So inorder to let the power of the latter two method, I can just make the distribution far away from distribution and the nonparametric method will of course be better than the parametric method.
To be more illustrated, I will use more example to show when the data is fram Normal distribution , the power of the Pearson test is the strongest.

```{r}
m = 1e3
for(i in 1:5){
  Sigma = matrix(c(1,i/10,i/10,1),2,2)
  result = matrix(1:(3*m), nrow = 3)
  for(i in 1:m){
    mnorm = mvrnorm(n = 100, rep(0,2), Sigma)
    x = mnorm[,1]
    y = mnorm[,2]
    result[1,i] = cor.test(x,y, alternative = "two.sided", method = 'pearson')$p.value
    result[2,i] = cor.test(x, y, alternative = "two.sided", method = 'kendal')$p.value
    result[3,i] = cor.test(x, y, alternative = "two.sided", method = 'spearman')$p.value
  }
  for(i in 1:3){
    print(mean(result[i,]<=0.05))
  }
}
```


##This is the work in 2018.11.2

###7.1
Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2


```{r 7.1}
library(bootstrap)
n = length(law$GPA)
cor.jack = numeric(n)
cor = cor(law$LSAT, law$GPA)
for(i in 1:n)
  cor.jack[i] = cor(law$LSAT[-i], law$GPA[-i])
bias.jack = (n-1)*(mean(cor.jack)-cor)
var.jack = (n-1)^2/n*var(cor.jack)
se.jack = sqrt(var.jack)
bias.jack
se.jack
```

*anaylysis*

We can compare our result with the result in bootstrap
```{r 7.1(2)}
B = 200
n = nrow(law)
R = numeric(B)
for(b in 1:B){
  i = sample(1:n, size = n, replace = TRUE)
  LSAT = law$LSAT[i]
  GPA = law$GPA[i]
  R[b] = cor(LSAT, GPA)
}
se.boot = sd(R)
bias.boot = mean(R)-cor
se.boot
bias.boot
```

the results are almost the same, so we can conclude that our result is right.

###7.5
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures 1/?? by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

```{r 7.5}
library(boot)
boot.mean = function(x,i) mean(x[i,])
ci.norm<-ci.basic<-ci.perc<-ci.bca<-matrix(NA,1,2)
de <- boot(data=aircondit,statistic=boot.mean, R = 999)
ci <- boot.ci(de,type=c("norm","basic","perc","bca"))
ci.norm[1,]<-ci$norm[2:3]
ci.basic[1,]<-ci$basic[4:5]
ci.perc[1,]<-ci$percent[4:5]
ci.bca[1,]<-ci$bca[4:5]
cat('norm =',ci.norm[1,], 'basic =',ci.basic[1,], 'perc =',ci.perc[1,], 'BCa =',ci.bca[1,])
```

*analysis*
Let's do an interesting simulation. We generate random numbers from $exp(\lambda)$, then compare the coverage rate of the above four intervals.

```{r 7.5(2)}
lambda = 0.1
m = 1e2
n = 1e2 ##iteration numbers
data.simu = rexp(m, lambda)
cover.norm = cover.basic = cover.perc = cover.bca =0
```

```{r }
boot.mean = function(x,i) mean(x[i])
for(i in 1:n){
  de <- boot(data = data.simu,statistic=boot.mean, R = 999)
  ci <- boot.ci(de,type=c("norm","basic","perc","bca"))
  ci.norm[1,]<-ci$norm[2:3]
  ci.basic[1,]<-ci$basic[4:5]
  ci.perc[1,]<-ci$percent[4:5]
  ci.bca[1,]<-ci$bca[4:5]
  cover.norm = cover.norm+I(ci.norm[1,1]<=1/lambda & 1/lambda<=ci.norm[1,2])
  cover.basic = cover.basic+I(ci.basic[1,1]<=1/lambda & 1/lambda<=ci.basic[1,2])
  cover.perc = cover.perc+I(ci.perc[1,1]<=1/lambda & 1/lambda<=ci.perc[1,2])
  cover.bca = cover.bca+I(ci.bca[1,1]<=1/lambda & 1/lambda<=ci.bca[1,2])
}
cover.norm/n
cover.basic/n
cover.perc/n
cover.bca/n
```
Well it's hard to find the difference of the four intervals in this simulation.

But I am willing to believe that the BCa will perform better at most of the time. Since other interval has some assumpations of the data. And the BCa interval also compensate other things.

###7.8
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standaed error of $\hat \theta$.

```{r 7.8}
co = cov(scor)
n = length(scor$mec)
eig = eigen(co)$values
theta = eig[1]/sum(eig)
theta.jack = numeric(n)
for(i in 1:n){
  co = cov(scor[-i,])
  eig = eigen(co)$values
  theta.jack[i] = eig[1]/sum(eig)
}
bias.jack = (n-1)*(mean(theta.jack)-theta)
var.jack = (n-1)^2/n*var(theta.jack)
se.jack = sqrt(var.jack)
bias.jack
se.jack
```

*analysis*
We can hope the jackknife can work because the statistic it estimates is smooth. The assumption which make the jackknife work is satisfied in this problem since the covariance is a smooth statistic.
And also I will compare it to the bootstrap method.
```{r 7.8(2)}
B = 200
theta.boot = numeric(B)
for(j in 1:B){
  i = sample(1:n, n, replace = TRUE)
  co = cov(scor[i,])
  eig = eigen(co)$values
  theta.boot[j] = eig[1]/sum(eig)
}
bias.boot = mean(theta.boot)-theta
se.boot = sd(theta.boot)
bias.boot
se.boot
```

So it turns out that our result is correct.

###7.11
In example 7.18, leave-one-out(n-fold)cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

```{r 7.11}
library(DAAG)
chemical = ironslag$chemical
magnetic = ironslag$magnetic
n = length(magnetic)
e1 <- e2 <- e3 <- e4 <- matrix(0, nrow = n, ncol = n)
```

```{r}
for(i in 1:(n-1))
  for(j in (i+1):n){
    y = magnetic[c(-i, -j)]
    x = chemical[c(-i, -j)]
    J1 = lm(y ~ x)
    yhat1 = J1$coef[1]+J1$coef[2]*chemical[c(i,j)]
    e1[i,j] = mean((magnetic[c(i,j)]-yhat1)^2)
    
    J2 = lm(y ~ x+I(x^2))
    yhat2 = J2$coef[1]+J2$coef[2]*chemical[c(i,j)]+J2$coef[3]*chemical[c(i,j)]^2
    e2[i,j] = mean((magnetic[c(i,j)]-yhat2)^2)
    
    J3 = lm(log(y) ~ x)
    logyhat3 = J3$coef[1]+J3$coef[2]*chemical[c(i,j)]
    yhat3 = exp(logyhat3)
    e3[i,j] = mean((magnetic[c(i,j)]-yhat3)^2)
    
    J4 = lm(log(y) ~ log(x))
    logyhat4 = J4$coef[1]+J4$coef[2]*log(chemical[c(i,j)])
    yhat4 = exp(logyhat4)
    e4[i,j] = mean((magnetic[c(i,j)]-yhat4)^2)
    
  }
c(2/(n*(n-1))*sum(e1),2/(n*(n-1))*sum(e2),2/(n*(n-1))*sum(e3),2/(n*(n-1))*sum(e4))

```

*analysis*
Through this result , we can conclude that we will choose the second model, which is conincide with the leave-one-out cross-validation. So we can also use leave-two-out validation to do the model selection.

##This is the work in 2018.11.16


###8.1

Implement the twp-sample Cramer-con Mises test for equal distribution permutation test. Apply the test to the data in Example 8.1 and 8.2

```{r 1(1)} 
attach(chickwts)
X = sort(as.vector(weight[feed == "soybean"]))
Y = sort(as.vector(weight[feed == 'linseed']))
R = 999
z = c(X,Y)
K = 1:26
n = length(X)
m = length(Y)
reps = numeric(R)
```

```{r 1(2)}
CMstat = function(x,y){
  x = sort(x)
  y = sort(y)
  F = function(x0){
    
    findInterval(x0, x)/n
  }
  G = function(y0){
    findInterval(y0, y)/m
  }
  m*n/(m+n)^2*(sum((F(x)-G(x))^2)+sum((F(y)-G(y))^2))
}
for(i in 1:R){
  k = sample(K, size = n, replace = FALSE)
  x1 = z[k]
  y1 = z[-k]
  reps[i] = CMstat(x1, y1)
  t0 = CMstat(X,Y)
  p.value = mean(c(t0, reps)>=t0)
}
p.value
```

###2

Design experiments for evaluating the performance of the NN, energy and ball methods in different situations.

*1. unequal variance but equal expectations*

```{r 2(1)}
library(RANN)
library(boot)
library(energy)
library(Ball)
n1 = 50
n2 = 30
p = 2
m = 200
k = 3
R = 200
n = n1+n2
N = c(n1, n2)

Tn = function(z, ix, sizes, k){
  n1 = sizes[1]
  n2 = sizes[2]
  z = z[ix, ]
  NN = nn2(data = z, k = k+1)
  block1 = NN$nn.idx[1:n1, -1]
  block2 = NN$nn.idx[(n1+1):n, -1]
  i1 = sum(block1 < n1+.5)
  i2 = sum(block2 > n1+.5)
  (i1+i2)/(k*n)
}

eqdist.nn = function(z, sizes, k, R){
  boot.obj = boot(data = Z, statistic = Tn, R = R, sim = 'permutation', sizes = sizes, k = 3)
  ts = c(boot.obj$t0, boot.obj$t)
  p.value = mean(ts>=ts[1])
  list(statistic = ts[1], p.value = p.value)
}

p.values = matrix(NA, m ,3)
for(i in 1:m){
  X = matrix(rnorm(n1*p), ncol = p)
  Y = cbind(rnorm(n2), rnorm(n2, sd = 1.5))
  Z = rbind(X, Y)
  p.values[i,1] = eqdist.nn(Z, sizes = N, k, R = R)$p.value
  p.values[i,2] = eqdist.etest(Z, sizes = N, R = R)$p.value
  p.values[i,3] = bd.test(x = X, y = Y, R = R, seed = i)$p.value
}
alpha = 0.05
pow = colMeans(p.values<alpha)
pow
```

From the result we con conclude that the Ball method is more powerful than the other two.

*2. unequal mean and unequal variance*

```{r 2(2)}
for(i in 1:m){
  X = matrix(rnorm(n1*p), ncol = p)
  Y = cbind(rnorm(n2, mean = 0.5), rnorm(n2, sd = 1.5))
  Z = rbind(X, Y)
  p.values[i,1] = eqdist.nn(Z, sizes = N, k, R = R)$p.value
  p.values[i,2] = eqdist.etest(Z, sizes = N, R = R)$p.value
  p.values[i,3] = bd.test(x = X, y = Y, R = R, seed = i)$p.value
}
alpha = 0.05
pow = colMeans(p.values<alpha)
pow
```

Then we will see what would happen if we narrow down the difference of the mean.

```{r 2(2)_2}
for(i in 1:m){
  X = matrix(rnorm(n1*p), ncol = p)
  Y = cbind(rnorm(n2, mean = 0.1), rnorm(n2, sd = 1.5))
  Z = rbind(X, Y)
  p.values[i,1] = eqdist.nn(Z, sizes = N, k, R = R)$p.value
  p.values[i,2] = eqdist.etest(Z, sizes = N, R = R)$p.value
  p.values[i,3] = bd.test(x = X, y = Y, R = R, seed = i)$p.value
}
alpha = 0.05
pow = colMeans(p.values<alpha)
pow
```

We can see as the difference of the mean becomes much smaller, the differece of the variance becomes dominate, than the power of the Ball extimation leads again.

To make the result more significant, we will change the parameter as follows.

```{r 2(2)_3}
for(i in 1:m){
  X = matrix(rnorm(n1*p), ncol = p)
  Y = cbind(rnorm(n2, mean = 0.3, sd = 1.3), rnorm(n2, sd = 1.5))
  Z = rbind(X, Y)
  p.values[i,1] = eqdist.nn(Z, sizes = N, k, R = R)$p.value
  p.values[i,2] = eqdist.etest(Z, sizes = N, R = R)$p.value
  p.values[i,3] = bd.test(x = X, y = Y, R = R, seed = i)$p.value
}
alpha = 0.05
pow = colMeans(p.values<alpha)
pow
```
      
In this example, we can see the power of the Ball parameter is much higher than the other two, this shows the advantage of the Ball parameter, more sensitive to the change of variance.

*3. Non-normal distributions*

t distribution with different degree of freedom
```{r 3(1)}
for(i in 1:m){
  X = matrix(rt(n1*p, df = 1), ncol = p)
  Y = cbind(rt(n2, df = 2), rt(n2, df = 3))
  Z = rbind(X, Y)
  p.values[i,1] = eqdist.nn(Z, sizes = N, k, R = R)$p.value
  p.values[i,2] = eqdist.etest(Z, sizes = N, R = R)$p.value
  p.values[i,3] = bd.test(x = X, y = Y, R = R, seed = i)$p.value
}
alpha = 0.1
pow = colMeans(p.values<alpha)
pow
```

From the result we can see that the Ball statistic is much more powerful than the other two.

Since t distribution with differnet means are somewhat very likely to the normal distribution with diffenret mean, so we omit it here

Mixture Normal

First, unequal mixture probability and equal mean and eqaul variance
```{r 3(2)}
library(MASS)
for(i in 1:m){
  p1 = 0.7
  p2 = 0.3
  U1 = runif(n1)
  U2 = runif(n2)
  X = ((U1<p1)*mvrnorm(n1, mu = c(0,0), Sigma = diag(c(1,1)))+(1-(U1<p1))*mvrnorm(n1, mu = c(1,1), Sigma = matrix(c(2,1.3,1.3,1), nrow = 2)))
  Y = ((U2<p2)*mvrnorm(n2, mu = c(0,0), Sigma = diag(c(1,1)))+(1-(U2<p2))*mvrnorm(n2, mu = c(1,1), Sigma = matrix(c(2,1.3,1.3,1), nrow = 2)))
  Z = rbind(X, Y)
  p.values[i,1] = eqdist.nn(Z, sizes = N, k, R = R)$p.value
  p.values[i,2] = eqdist.etest(Z, sizes = N, R = R)$p.value
  p.values[i,3] = bd.test(x = X, y = Y, R = R, seed = i)$p.value
}
alpha = 0.1
pow = colMeans(p.values<alpha)
pow
```

We can see that the Energy method and the Ball method has comparable power, and the power of NN is much lower.

equal mixture probability, but different mean and variance

```{r 3(3)}
library(MASS)
for(i in 1:m){
  p1 = 0.4
  p2 = 0.4
  U1 = runif(n1)
  U2 = runif(n2)
  X = ((U1<p1)*mvrnorm(n1, mu = c(0,0), Sigma = diag(c(1,1)))+(1-(U1<p1))*mvrnorm(n1, mu = c(1,1), Sigma = matrix(c(1,0,0,1), nrow = 2)))
  Y = ((U2<p2)*mvrnorm(n2, mu = c(0.5,0), Sigma = diag(c(2,1)))+(1-(U2<p2))*mvrnorm(n2, mu = c(1,1.2), Sigma = matrix(c(2,1.3,1.3,1), nrow = 2)))
  Z = rbind(X, Y)
  p.values[i,1] = eqdist.nn(Z, sizes = N, k, R = R)$p.value
  p.values[i,2] = eqdist.etest(Z, sizes = N, R = R)$p.value
  p.values[i,3] = bd.test(x = X, y = Y, R = R, seed = i)$p.value
}
alpha = 0.1
pow = colMeans(p.values<alpha)
pow
```

We can see that in this situation, the NN method finnaly makes it the best one, while the Ball method and the energy method has similar power.

*4. unbalanced samples*

```{r 4}
n1 = 100
n2 = 10
N = c(n1, n2)
n = n1+n2
for(i in 1:m){
  X = matrix(rnorm(n1*p), ncol = p)
  Y = cbind(rnorm(n2), rnorm(n2, mean = 0.5,sd = 1.8))
  Z = rbind(X, Y)
  p.values[i,1] = eqdist.nn(Z, sizes = N, k, R = R)$p.value
  p.values[i,2] = eqdist.etest(Z, sizes = N, R = R)$p.value
  p.values[i,3] = bd.test(x = X, y = Y, R = R, seed = i)$p.value
}
alpha = 0.1
pow = colMeans(p.values<alpha)
pow
```

I try many parameters but hard to find significance difference between the three method.
And the result above is a rather significant result compared to  other result.
It seems the energy test can show more power when the number of samples is rather unbalanced.
And it turns out that it's a rather tough task for all the three methods, the powers they show are much smaller than when the sample is balanced.

*Analysis*

So in conclusion, the Ball method usually shows much more powerful than the other two methods, especially when we are under the situation the variance is different or the distribution has a heavy tail. And we can see that NN usually shows the leat powerful performance. And the Energy method's power is usually betweeen the other two, and sometimes can be the top one, like the the last situation and the mixture model situation.

###9.3

Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the ???rst 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchy or qt with df=1). Recall that a Cauchy(??,??) distribution has density function
$f(x) = \frac{1}{\theta\pi(1+\frac{x-\eta}{\theta})^2}$
The standard Cauchy has the Cauchy(?? =1,??= 0) density. (Note that the standard Cauchy density is equal to the Student t density with one degree of freedom.) 

```{r}
burnin = 4000
N = 100000
rw.Metropolis = function(n, sigma, x0, N){
  x = numeric(N)
  x[1] = x0
  u = runif(N)
  k = 0
  for(i in 2:N){
    y = rnorm(1, x[i-1], sigma)
    if(u[i]<=(dt(y, n)/dt(x[i-1], n))){
      x[i] = y
      k = k+1
    }else{
      x[i] = x[i-1]
    }
  }
  return(list(x = x, k=k))
}

result = rw.Metropolis(n = 1, sigma = 0.9, x0 = 25, N = N)
simulation = result$x[(burnin+1):N]

quant.simu = quantile(simulation, (5:95)/100)
quant.true = qcauchy((5:95)/100)
plot(quant.simu, quant.true)
g = function(x) x
curve(g, -6,6, add = TRUE, )
```

There is a slight deviation between in the hvery high percentile or very low percentile, I think the reason for this problem is that the data in these area are very sparse, since we use the sample quantile to approximate the true quantile , it will have some deviations. And the most important reason is that the $t_1$ distribution has very heavy tails(it even don't have the expectation), so the high tail or low tail estimation will be very insteady.
Even if we generate data from the true distribution, there will be some deviations, I will demonstrate below.


```{r }
data = rt(10000, df = 1)
quant.data = quantile(data, (5:95)/100)
quant.true = qcauchy((5:95)/100)
plot(quant.simu, quant.true)
g = function(x) x
curve(g, -6,6, add = TRUE, )
```
That's exactly the figure we get above.

Throught the figure we may draw the conclusion that the quantile between the simulation and the true quantile is the same, but to be more rigorous, I use the least sqaure method to find the slope.

```{r }
result = lm(quant.true~quant.simu)
result$coefficients
```
What if we narrow down the quantile.

```{r}
quant.simu = quantile(simulation, (10:90)/100)
quant.true = qcauchy((10:90)/100)
result = lm(quant.true~quant.simu)
result$coefficients
```
The result becomes more attractive.


###9.6

Rao presented an example on genetic linkage of 197 animals in four categories. The group sizes are(125, 18, 20, 34)??Assume that the probabilities of the corresponding multinomial distribution are $(\frac{1}{2}+\frac{\theta}{4}, \frac{1-\theta}{4}, \frac{1-\theta}{4}, \frac{\theta}{4})$

```{r}
N = 10000
burnin = 2000
w = 1
x = numeric(N)
x[1] = 0.5
f <- function(y) {
  I(y>0 & y<1)*(0.5 + y/4)^125 * ((1-y)/4)^18 * ((1-y)/4)^20 * (y/4)^34
}
for (i in 2:N) {
  k = 0
  u = runif(1)
  x1 = runif(1, 0, w)
  if(u < f(x1)/f(x[i-1])){
    x[i] = x1
    k = k+1
  }
  else x[i] = x[i-1]
}

xp = x[(burnin+1):N]
beta = mean(xp)
print(round(c(0.5 + beta/4, (1-beta)/4, (1-beta)/4, beta/4), 3))
print(round(c(125, 18, 20, 34)/sum(c(125, 18, 20, 34)),3))

hist(xp, freq = F)
```

In order to check our result is true, we first see the process of x

```{r}
plot(x, type = "l", xlab = "x", xlim = c(1, N), ylim = range(x))
```

And we calculate the MML, to see whether it is equal to the posterior mean

```{r}
nlm(f = function(beta){
  -I(beta>0 & beta<1)*(125*log(0.5+beta/4)+38*log((1-beta)/4)+34*log(beta/4))
}, p = 0.5)$estimate
beta
```      
They are very close, and if the sample number get much lager, they will become closer and closer, since the prior information is becoming less and less significant.

##This is the work in 2018.11.23

###9.6
Monitoring convergence

```{r}
N = 2100
K = 50
burnin = 2000
n = N-burnin
w = 1
x = matrix(NA, nrow = K, ncol = N)
x[,1] = 0.5
f <- function(y) {
  I(y>0 & y<1)*(0.5 + y/4)^125 * ((1-y)/4)^18 * ((1-y)/4)^20 * (y/4)^34
}
for (i in 2:N) {
  u = runif(K)
  x1 = runif(K, 0, w)
  flag = u<f(x1/x[,(i-1)])
  x[,i] = I(u<f(x1)/f(x[,(i-1)]))*x1+I(u>=f(x1)/f(x[,(i-1)]))*x[,(i-1)]
}
samp = x[,(burnin+1):N]
row.mean = rowMeans(samp)
B_n = var(row.mean)*n
W_n = mean((samp-row.mean)^2)
Phi = (n-1)/n*W_n+1/n*B_n
R = sqrt(Phi/W_n)
R
```

I just randomly select some burnin value and N value, and see that after 2000 steps the chain has mixed very well, and converges.

###11.4

Find the intersection points A(k) in (0, $\sqrt k$), of the curves
$S_{k-1}(a) = P(t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}})$ and
$S_k(a) = P(t(k)>\sqrt{\frac{a^2k}{k+1-a^2}})$
for k = 4:25, 100, 500, 1000, where t(k) is a Student k random variable with k degrees of freedom.

```{r 11.4}
k = c(4:25, 100, 500, 1000)
a = numeric(length(k))
for(i in (1:length(k))){
  f = function(a){
    pt(sqrt(a^2*(k[i]-1)/(k[i]-a^2)), df = k[i]-1, lower.tail = FALSE, log.p = T) / pt(sqrt(a^2*k[i]/(k[i]+1-a^2)), df = k[i], lower.tail= FALSE, log.p = T) -1
  }
  a[i] = uniroot(f, c(0.001, sqrt(k[i])-0.00001))
}
print(a)
```

*Analysis*

At first, if I do not add the log.p = T, when the k gets larger and larger, the result is not correct, especially in the tail of the k, it will approximate the boundary value, which is undesirable compared to the result when the k is smaller. However, if we add log.p = T, the result is much better. So it's important to add the log.p = T, if we calculate the pt() first, then add log() to it, it is unusable, the problem lies in the accuracy of the calculation. We can run a simple example to show the way the log.p = T works.

```{r}
log(1e-330)
log(1e-115)+log(1e-115)
```

```{r}
1e-330-1e-331
log(1e-330)-log(1e-331)
log(1e-115)+log(1e-115)-(log(1e-115)+log(1e-116))
```

So in the example above, we may see how log.p = T solve the problem.

##This is the work in 2018.11.30

###A-B-O blood type problem

Well this is the work that makes everybody struggle, but I truly learn a lot in this work. At least, I learned how to use the right EM algorithm.

  + Let the three alleles be A, B, and O.
```{r,echo=FALSE, eval=TRUE}
dat <- rbind(Genotype=c('AA','BB','OO','AO','BO','AB','Sum'),
              Frequency=c('p^2','q^2','r^2','2pr','2qr','2pq',1),
              Count=c('nAA','nBB','nOO','nAO','nBO','nAB','n'))
knitr::kable(dat,format='latex')
```

      + Observed data: $n_{A\cdot}=n_{AA}+n_{AO}=28$ (A-type), $n_{B\cdot}=n_{BB}+n_{BO}=24$ (B-type), $n_{OO}=41$ (O-type), $n_{AB}=70$ (AB-type).
    
      + Use EM algorithm to solve MLE of $p$ and $q$ (consider missing data $n_{AA}$ and $n_{BB}$).
    
      + Record the log-maximum likelihood values in M-steps, are they increasing?
      
```{r}
nA_ = 28; nB_ = 24; nOO = 41;nAB = 70
l = c(NULL)

ll = function(p,q,r){
  -(nA_*log(p^2+2*p*r)+nB_*log(q^2+2*q*r)+2*nOO*log(r)+nAB*log(2*p*q))
}

pi1 = 0.2
pi2 = 0.2
nAA = nA_*pi1
nBB = nB_*pi2
nAO = nA_-nAA
nBO = nB_-nBB
p = 2*nAA+nAO+nAB
q = 2*nBB+nBO+nAB
r = 2*nOO+nAO+nBO
sum = p+q+r
p = p/sum
q = q/sum
r = r/sum

print(c(p,q,r))

l = c(l, ll(p,q,r))


EM = function(tol = 1e-12){
  iter = 1
  flag = TRUE
  while(flag == TRUE){
    pi1 = p/(p+2*r)
    pi2 = q/(q+2*r)
    nAA = nA_*pi1
    nBB = nB_*pi2
    nAO = nA_-nAA
    nBO = nB_-nBB
    p.new = 2*nAA+nAO+nAB
    q.new = 2*nBB+nBO+nAB
    r.new = 2*nOO+nAO+nBO
    sum = p.new+q.new+r.new
    p.new = p.new/sum
    q.new = q.new/sum
    r.new = r.new/sum
    flag = max(abs(p.new-p), abs(q.new-q), abs(r.new-r))>tol
    iter = iter+1
    p = p.new
    q = q.new
    r = r.new
    l = c(l, ll(p,q,r))
  }
  list(p = p, q = q, r = r, iter = iter, ll = l)
}

result = EM()

result

plot(result$ll)

```

*Analysis*

I'll share my idea. We can see that if we know all the nAA, nBB or things like that, we can easily derive the loglikehood, and we can easily solve this problem, but here we are only provided things like nA_, nB_, so the hidden variable is nAA and nBB or we can say is the probability of nAA in nA_ and probability of nBB in bB_ and I use thses as hidden variables. The things I describe above is the E step, and next I will describe M step. When we estimated the pi1 and pi2, we can use this to get nAA and nBB, so we can get the full likelihood function, and then we can get the estimation of p,q,r using MLL method. And finnaly I use the change of parameter between two steps as our stop criteria.

We can also use the change between the negative log likelihood as my criteria, which is more rigorous. 

And since there is a theoretical guarantee of EM method which guarantee the decrease of negative log likelihood, and through our simulation, we can prove this.

Compare our result with the maximum likelihood.

```{r}
nlm(f = function(beta){
  p = beta[1]; q = beta[2]; r = 1-p-q
  -(nA_*log(p^2+2*p*r)+nB_*log(q^2+2*q*r)+2*nOO*log(r)+nAB*log(2*p*q))
}, p = c(0.4,0.3))
```
They are almost the same, so I can conclude that my EM method converges to the true parameter.

###11.6

Write a function to compute the cdf of the Cauchy distribution, which has density

where??>0. Compare your results to the results from the R function pcauchy. (Also see the source code in pcauchy.c.)

```{r, eval = FALSE}
eta = 0
theta = seq(1, 10, length.out = 10)
n = 100
f = function(x, eta, theta){
  1/(theta*pi*(1+((x-eta)/theta)^2))
}
v = matrix(NA, length(theta), n)
q = seq(-20, 20, length.out = n)
for (i in 1:length(theta)) {
  for (j in 1:n) {
    v[i, j] = integrate(f, lower = -Inf, upper = q[j], theta = theta[i], eta)$value
  }
}
par(mfrow = c(4, 5))
g = function(x) {x}
for (i in 1:length(theta)) {
  qc = pcauchy(q, scale = theta[i])
  qqplot(qc, v[i, ], xlab = "Cauchy Quantiles", ylab = "Integration Quantiles")
  curve(g, add = T, type = "l", col = "red")
}
```

In building pdf, this chunck suffers from a problem and I don't know why, so I don't evaluate it.

We can see that our numerical integration do get the right calculation.

##This is the work in 2018.12.7
### p204 3
Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:
formulas<-list(
  mpg ~ disp,
  mpg ~ I(1/disp),
  mpg ~ disp+wt,
  mpg ~ I(1/disp)+wt
)

```{r}
attach(mtcars)
formulas<-list(
  mpg ~ disp,
  mpg ~ I(1/disp),
  mpg ~ disp+wt,
  mpg ~ I(1/disp)+wt
)
par(mfrow = c(2,2))

models1 = lapply(formulas, lm)
print(models1)

for(item in formulas)
  print(lm(item))

```

We can easily see they give the same result and the first solution is quicker to code.


###p204 4
Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply(). Can you do it without an anonymous function?

bootstraps <- lapply(1:10, function(i) {   rows <- sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
}) 

```{r}
bootstraps <- lapply(1:10, function(i) {   rows <- sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
}) 

for(i in 1:10){
  mpg = bootstraps[[i]]$mpg
  disp = bootstraps[[i]]$disp
  print(lm(mpg~disp))
}

models2 = lapply(bootstraps, function(o){
  lm(o$mpg~o$disp)})
print(models2)
```
We can see they give the same result.

```{r}
replicate(10, expr = {
  rows <- sample(1:nrow(mtcars), rep = TRUE)
  a = mtcars[rows, ]
  mpg = a$mpg
  disp = a$disp
  lm(mpg~disp)
})
```

We can use replicate to get the same result.

###p204 5

For each model in the previous two exercises,extract R2 using the function below.
rsq <- function(mod) summary(mod)$r.squared

```{r}
rsq <- function(mod) summary(mod)$r.squared
lapply(models1, rsq)
lapply(models2, rsq)
```

Use lapply we can easily get the result.

###p213 3

The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)), simplify = FALSE ) 

```{r}
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
sapply(trials, function(o) o$p.value)
```

Use sapply we can easily get the result.

Extra challenge:
```{r}
for(i in 1:100)
  print(trials[[i]]$p.value)
```

###p214 6

Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

```{r eval = FALSE}
library(parallel)
mcvMap <- function(f, FUN.VALUE , ...) {
    out <- mcMap(f, ...)
    vapply(out, identity, FUN.VALUE)
}
```

If your computer is windows, you should use foreach library


##This is the work in 2018.12.14

###p365 4

Make a faster version of chsiq.test() that only computes the chi-sqaure statistic when the input is two numeric vectors with no missing values. You can try simplifying chisq.test() or by coding from the mathematical de???nition.


```{r}
chisq.test1 <- function(x, y){
  m = table(x, y)
  margin1 <- rowSums(m)
  margin2 <- colSums(m)
  n <- sum(m)
  me <- tcrossprod(margin1, margin2) / n
  x_stat = sum((m - me)^2 / me)
  x_stat
}

```

We can first check our implementation gives the right answer.

```{r}
x = sample(1:3, replace = TRUE, size = 50)
y = sample(1:3, replace = TRUE, size = 50)
chisq.test1(x, y)
chisq.test(x,y)
```

And We can also Finally we can use microbenchmark to compare this implementation against the original 

```{r}
library(microbenchmark)
microbenchmark(
  suppressWarnings(chisq.test(x,y)),
  chisq.test1(x,y)
)
```
We can find that my method is much quicker.

*One thing to Note*

And we should use suppressWarnings() to compare the speed,otherwise, the warnings to slow down the chisq.test(), so we cannot get the right speed.

###p365 5

Can you make a faster version of table() for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test?


```{r }
table1 = function(x,y){
  rowname = unique(x)
  colname = unique(y)
  a = matrix(0, length(rowname), length(colname), dimnames = list(rowname, colname))
  for(i in seq_along(x)){
    m = which(rowname == x[i])
    n = which(colname == y[i])
    a[m,n] = a[m,n]+1
  }
  
  class(a) = 'table'
  a
}
```


First, test the correctness

```{r}
a = 1:5
b = sample(1:5)
table(a,b)
table1(a,b)
```

We can find that they are the same

Then test for the speed

```{r}
library(microbenchmark)
microbenchmark(
  table(x,y),
  table1(x,y)
)
```

My method is quicker.

However, there is still ways to improve my method, such as use Rcpp to do the loop, this will definitely increase the speed. But here we will omit it, since our method is better than the original.

Use table1 to make my chisq.test faster.

```{r}
chisq.test2 <- function(x, y){
  m = table1(x, y)
  margin1 <- rowSums(m)
  margin2 <- colSums(m)
  n <- sum(m)
  me <- tcrossprod(margin1, margin2) / n
  x_stat = sum((m - me)^2 / me)
  x_stat
}
```

```{r test the speed}
microbenchmark(
  suppressWarnings(chisq.test(x,y)),
  chisq.test1(x,y),
  chisq.test2(x,y)
)
```

We can see through the new version of table1() our method is quicker.

###Additional 
Write an Rcpp function for Exercise 9.6 (page 277, Statistical Computing with R). Compare the corresponding generated random numbers with those by the R function you wrote before using the function “qqplot”. Campare the computation time of the two functions with the function “microbenchmark”. Comments your results.

```{r}
N = 10000
burnin = 2000
RMCMC = function(N){
  x = numeric(N)
  x[1] = 0.5
  f <- function(y) {
    I(y>0 & y<1)*(0.5 + y/4)^125 * ((1-y)/4)^18 * ((1-y)/4)^20 * (y/4)^34
  }
  for (i in 2:N) {
    k = 0
    u = runif(1)
    x1 = runif(1)
    if(u < f(x1)/f(x[i-1])){
      x[i] = x1
      k = k+1
    }
    else x[i] = x[i-1]
  }
  x
}
x = RMCMC(N)
xp = x[(burnin+1):N]
hist(xp)
q1 = quantile(xp, seq(0.05,0.95,0.025))
```


```{r, eval = FALSE}
library(Rcpp)
sourceCpp('/Users/Daniel/Desktop/MCMC.cpp')


y = numeric(N)
MCMC(y, N)
hist(y)
yp = y[(burnin+1):N]
q2 = quantile(yp, seq(0.05,0.95,0.025))
qqplot(q1,q2)
g = function(x)
  x
curve(g, from = 0.05, to = 0.95, add = TRUE, col = 'red')
```

```{r, eval=FALSE}
library(microbenchmark)
microbenchmark(
  x = RMCMC(1000),
  MCMC(y,1000)
)
```
      
I run this example in my friend's laptop, and it cannot run in my laptop, so I will not evaluate these chunk.



